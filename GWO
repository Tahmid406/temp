import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
from sklearn.metrics import f1_score

# Path to ground truth masks
ground_truth_masks_path = r'D:\Tahmid\Data\Test\fold_4\masks'

# Global variable for the class to evaluate (will be updated in the loop)
class_id = 1  

class ConstrainedGWO:
    def __init__(self, obj_func, dim=10, search_domain=(0, 1), population_size=50, max_iter=100):
        self.obj_func = obj_func
        self.dim = dim
        self.search_domain = search_domain
        self.population_size = population_size
        self.max_iter = max_iter
        
        # Best solutions found so far (3 leaders: alpha, beta, delta)
        self.alpha_pos = None
        self.alpha_score = float('inf')
        
        self.beta_pos = None
        self.beta_score = float('inf')
        
        self.delta_pos = None
        self.delta_score = float('inf')
        
        # Initialize solution tracking
        self.best_scores = []
        self.final_solution = None
        
    def _normalize_solution(self, solution):
        """Normalize solution so sum equals 1"""
        sum_val = np.sum(solution)
        if sum_val > 0:  # Avoid division by zero
            return solution / sum_val
        else:
            # If all values are zero, return a uniform distribution
            return np.ones(self.dim) / self.dim
    
    def _initialize_population(self):
        """
        Initialize the wolf pack population with random solutions
        that satisfy the sum constraint
        """
        population = np.random.random((self.population_size, self.dim))
        
        # Normalize each solution to sum to 1
        for i in range(self.population_size):
            population[i] = self._normalize_solution(population[i])
            
        return population
    
    def _update_position(self, current_pos, a, A1, A2, A3, C1, C2, C3):
        # Position update components from alpha, beta, and delta
        d_alpha = abs(C1 * self.alpha_pos - current_pos)
        d_beta = abs(C2 * self.beta_pos - current_pos)
        d_delta = abs(C3 * self.delta_pos - current_pos)
        
        x1 = self.alpha_pos - A1 * d_alpha
        x2 = self.beta_pos - A2 * d_beta
        x3 = self.delta_pos - A3 * d_delta
        
        # New position is the average of positions from alpha, beta, and delta influence
        new_pos = (x1 + x2 + x3) / 3
        
        # Ensure solutions are within bounds
        new_pos = np.clip(new_pos, self.search_domain[0], self.search_domain[1])
        
        # Normalize to satisfy the sum = 1 constraint
        new_pos = self._normalize_solution(new_pos)
        
        return new_pos
    
    def optimize(self, verbose=True):
        # Initialize the population
        population = self._initialize_population()
        fitness = np.zeros(self.population_size)
        
        # Main loop
        for iteration in range(self.max_iter):
            # Evaluate each wolf's fitness
            for i in range(self.population_size):
                fitness[i] = self.obj_func(population[i])
                print(f"Iteration {iteration + 1}, Wolf {i + 1}: Fitness = {fitness[i]}")
                
                # Update alpha, beta, and delta
                if fitness[i] < self.alpha_score:
                    self.alpha_score = fitness[i]
                    self.alpha_pos = population[i].copy()
                elif fitness[i] < self.beta_score:
                    self.beta_score = fitness[i]
                    self.beta_pos = population[i].copy()
                elif fitness[i] < self.delta_score:
                    self.delta_score = fitness[i]
                    self.delta_pos = population[i].copy()
            
            # Update a parameter (decreases linearly from 2 to 0)
            a = 2 - iteration * (2 / self.max_iter)
            
            # Update each wolf's position
            for i in range(self.population_size):
                r1, r2 = np.random.random(2)
                A1 = 2 * a * r1 - a
                C1 = 2 * r2
                
                r1, r2 = np.random.random(2)
                A2 = 2 * a * r1 - a
                C2 = 2 * r2
                
                r1, r2 = np.random.random(2)
                A3 = 2 * a * r1 - a
                C3 = 2 * r2
                
                population[i] = self._update_position(
                    population[i], a, A1, A2, A3, C1, C2, C3
                )
            
            # Store best score for tracking
            self.best_scores.append(self.alpha_score)
            print(f"Iteration {iteration + 1}, Best Score: {self.alpha_score}")
        
        # Store the final solution
        self.final_solution = self.alpha_pos
        return self.alpha_pos, self.alpha_score
    
    def plot_convergence(self):
        """Plot the convergence curve of the optimization process"""
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, len(self.best_scores) + 1), self.best_scores)
        plt.title('GWO Convergence Curve')
        plt.xlabel('Iteration')
        plt.ylabel('Objective Function Value')
        plt.grid(True)
        plt.show()

def objective_function(weights):
    # Initialize paths and variables
    base_path = r'D:\Tahmid\Probability_Maps'
    npy_model_folders = [os.path.join(base_path, model) for model in os.listdir(base_path) 
                         if os.path.isdir(os.path.join(base_path, model))]
    
    dsc_scores = []
    
    # Get all ground truth mask files
    ground_truth_files = [f for f in os.listdir(ground_truth_masks_path) 
                          if f.endswith('.png') or f.endswith('.jpg')]
    
    for mask_file in ground_truth_files:
        ground_truth_mask_path = os.path.join(ground_truth_masks_path, mask_file)
        base_name = os.path.splitext(mask_file)[0]
        
        model_outputs = []
        all_models_exist = True
        
        for model_folder in npy_model_folders:
            npy_file_path = os.path.join(model_folder, f"{base_name}.npy")
            if not os.path.exists(npy_file_path):
                all_models_exist = False
                break
            probability_map = np.load(npy_file_path)
            model_outputs.append(probability_map)
        
        if not all_models_exist:
            continue
            
        weighted_probability_map = np.zeros_like(model_outputs[0])
        for i, output in enumerate(model_outputs):
            weighted_probability_map += weights[i] * output
        
        output_mask = (weighted_probability_map > 0.5).astype(int)
        ground_truth_mask = Image.open(ground_truth_mask_path).convert('P')
        ground_truth_mask = np.array(ground_truth_mask)
        
        num_classes = 7  # Adjust based on your number of classes
        ground_truth_mask_one_hot = np.eye(num_classes)[ground_truth_mask]
        ground_truth_mask_one_hot = ground_truth_mask_one_hot.transpose(2, 0, 1)
        
        # Calculate DSC score (F1 score) for the current class
        dsc_score = f1_score(ground_truth_mask_one_hot[class_id].flatten(), 
                             output_mask[class_id].flatten(), zero_division=0)
        if dsc_score > 0:
            dsc_scores.append(dsc_score)
    
    if dsc_scores:
        return -np.mean(dsc_scores)  # Negative for minimization
    else:
        return 0

if __name__ == "__main__":
    # Set random seed for reproducibility
    np.random.seed(42)
    dim = 10  # Problem dimension (number of parameters to optimize)
    
    # Loop over class IDs 1 to 6
    for current_class in range(1, 7):
        print(f"\n=== Processing Class {current_class} ===")
        # Update the global class_id for the objective function
        class_id = current_class
        
        # Create the GWO optimizer for the current class
        gwo = ConstrainedGWO(
            obj_func=objective_function,
            dim=dim,
            search_domain=(0, 1),
            population_size=50,
            max_iter=100
        )
        
        # Run the optimization
        best_solution, best_fitness = gwo.optimize(verbose=True)
        
        # Print results for the current class
        print("\nOptimization complete!")
        print(f"Class {class_id}:")
        print(f"Best solution: {best_solution}")
        print(f"Sum of parameters: {np.sum(best_solution)}")
        print(f"Best fitness: {best_fitness}")
        
        # Plot convergence curve
        gwo.plot_convergence()
        
        # Save the best solution weights to a file
        save_path = rf"D:\Tahmid\Utils\New folder\GWO_class{class_id}_weights.txt"
        np.savetxt(save_path, best_solution)
        print(f"Saved weights to {save_path}")
